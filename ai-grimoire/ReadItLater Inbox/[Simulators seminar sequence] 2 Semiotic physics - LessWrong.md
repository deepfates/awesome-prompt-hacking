[[ReadItLater]] [[Article]]

# [[Simulators seminar sequence] #2 Semiotic physics - LessWrong](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics)

Crossposted from the [AI Alignment Forum](https://alignmentforum.org/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics). May contain more technical jargon than usual.

***Meta:** Over the past few months, we've held a seminar series on the* [*Simulator theory*](https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators) *by janus. As the theory is actively under development, the purpose of the series is to uncover central themes and formulate open problems. A few high-level remarks upfront:*

-   *Our aim with this sequence is to share some of our discussions with a broader audience and to encourage new research on the questions we uncover.* 
-   *We outline the broader rationale and shared assumptions in* [*Background and shared assumptions*](https://www.alignmentforum.org/posts/nmMorGE4MS4txzr8q/simulators-seminar-sequence-1-background-and-shared)*. That article also contains general caveats about how to read this sequence - in particular, read the sequence as a collection of incomplete notes full of invitations for new researchers to contribute.*

***Epistemic status:** Exploratory. Parts of this text were generated by a language model from language model-generated summaries of a transcript of a seminar session. The content has been reviewed and edited for conceptual accuracy, but we have allowed many idiosyncrasies to remain.* 

## Three questions about language model completions

GPT-like models are driving most of the recent breakthroughs in natural language processing. However, we don't understand them at a deep level. For example, when GPT creates a completion like the [Blake Lemoine greentext](https://generative.ink/artifacts/lamda2/), we

1.  can't explain *why* it creates that exact completion.
2.  can't identify the properties of the text that predict how it continues.
3.  don't know how to affect these high-level properties to achieve desired outcomes.

![](ReadItLater%20Inbox/assets/emdxgnl6ch3fcdxzkkff.png)

We can make statements like "this token was generated because of the multinomial sampling after the softmax" or "this behavior is implied by the training distribution", but these statements only imply a form of [descriptive adequacy](https://www.wikiwand.com/en/Levels_of_adequacy) (or saying “AlphaGo **will** win this game of Go"). They don't provide any [explanatory adequacy](https://www.wikiwand.com/en/Levels_of_adequacy), which is what we need to sufficiently understand and make use of GPT-like models.

Simulator theory ([janus, 2022](https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators)) has the potential for explanatory adequacy for some of these questions. In this post, we'll explore what we call “semiotic physics”, which follows from simulator theory and which has the potential to provide partial answers to questions 1., 2. and perhaps 3. The term “semiotic physics” here refers to the **study of the fundamental forces and laws that govern the behavior of signs and symbols**. Similar to how the study of physics helps us understand and make use of the laws that govern the physical universe, semiotic physics studies the fundamental forces that govern the symbolic universe of GPT, a universe that reflects and intersects with the universe of our own cognition. We transfer concepts from dynamical systems theory, such as attractors and basins of attraction, to the semiotic universe and spell out examples and implications of the proposed perspective.

## Example. Semiotic coin flip.

To illustrate what we mean by semiotic physics, we will look at a toy model that we are familiar with from regular physics: coin flips. In this setup, we draw a sequence of coin flips from a large language model[\[1\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fn63rhsem9wb8). We encode the coin flips as a sequence of the strings  `1` and  `0` (since they are tokenized as a single token) and zero out all probabilities of other tokens. 

We can then look at the probability of the event  that the sequence of coin flips ends in tails ( `0`) or heads ( `1`) as a function of the sequence length.

![](ReadItLater%20Inbox/assets/vq423fthuyxzxd55ctjm.png)

We note two key differences between the semiotic coin flip and a fair coin:

-   the semiotic coin is not fair, i.e. it tends to produce sequences that end in tails ( `0`) much more frequently than sequences that end in heads ( `1`).
-   the semiotic coin flips are not independent, i.e. the probability of observing heads or tails changes with the history of previous coin flips.

To better understand the types of sequences that end in either tails or heads, we next investigate the probability of the most likely sequence ending in  `0` or  `1`. As we can see in the graph below, the probability of the most likely sequence ending in  `1` does not decrease for the GPT coin as rapidly as it does for a fair coin.

![](ReadItLater%20Inbox/assets/hz5knaltt4k2lqkfffrn.png)

Again, we observe a notable difference between the semiotic coin and the fair coin:

-   while the probability of a given sequence of coin flips decreases exponentially (every sequence of length  of fair coinflips has the same probability ), the probability of the most likely sequence of semiotic coin flips decreases much slower.

This difference is due to the fact that the most likely sequence of semiotic coinflips ending in f.e.  `0` is:  `0`  `0`  `0`  `0` ...  `0`  `0`. Once the language model has produced the same token four or five times in a row, it will latch onto the pattern and continue to predict the same token with high probability. As a consequence, the probability of the sequence does not decrease as drastically with increasing length, as each successive term has almost a probability of .

With the example of the semiotic coin flip in mind, we will set up some mathematical vocabulary for discussing semiotic physics and demonstrate how the vocabulary pays off with two propositions. We believe this terminology is primarily interesting for alignment researchers who would like to work on the theory of semiotic physics. The arithmophobic reader is invited to skip or gloss over the section (for an informal discussion, see [here](https://generative.ink/posts/language-models-are-multiverse-generators/#dynamics)).

## Simulations as dynamical systems

Simulator theory distinguishes between the simulator (the entity that performs the simulation) and the simulacrum (the entity that is generated by the simulation). The simulacrum arises from the chained application of the simulation forward pass. **The result can be viewed as a dynamical system where the simulator describes the system’s dynamics and the simulacrum is instantiated through a particular trajectory.**

We commence by identifying the *state* and *trajectory* of a dynamical system with tokens and sequences of tokens.

**Definition of the state and trajectories.** Given an alphabet of tokens  with cardinality , we call  the *state* of the model, and  the *trajectory*.[\[2\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrkz2ubpm6se) While a trajectory can generally be of arbitrary length, we denote the context length of the model as ; therefore,  can effectively be written as . The empty sequence is denoted as .[\[3\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fn3axt6v1g12)[\[4\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnh8z6w9i5f6)[\[5\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnox2rrio3qx8)

While token sequences are the objects of semiotic physics, the actual laws of semiotic physics derive from the simulator. In particular, a simulator will provide a distribution over the possible next state given a trajectory via a *transition rule*.

**Definition of the transition rule.** The transition rule is a random function that maps a trajectory to a probability distribution over the alphabet (i.e., the probabilities for the next token completion after the current state). Let  denote the set of [probability mass functions](https://en.wikipedia.org/wiki/Probability_mass_function) over , i.e., the set of functions  which satisfies the Kolmogorov axioms.[\[6\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnhhxowtlu14b)[\[7\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fn4eq2kwkwlf6)[\[8\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnppa8olzjrbl) The transition rule is then a function .

Analogous to the wave collapse in quantum physics, [sampling a new state from a distribution over states turn possibility into reality](https://generative.ink/posts/language-models-are-multiverse-generators/). We call this phenomenon the *sampling procedure*.

**Definition of the sampling procedure.** The *sampling procedure* , selects a next token, which is not ruled out by the transition rule, i.e., .[\[9\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnr8kij1lsct)  The resulting trajectory  is simply the concatenation of  and  (see the evolution operator below). We can, therefore, define the repeated application of the sampling procedure recursively as  and .

Lastly, we need to concatenate the newly sampled token to the trajectory of the previous token to obtain a new trajectory. Packaging the transition rule, the sampling procedure, and the concatenation results in the *evolution operator*, which is the main operation used for running a simulation.

**Definition of the evolution operator.** Putting the pieces together, we finally define the function  that evolves a given trajectory, i.e., transforms  into  by appending the token generated by the sampling procedure . That is,  is defined as . As above, repeated application is denoted by .

*Note that both the sampling procedure and the evolution operator are not functions in the conventional sense since they include a random element (the step of sampling from the distribution given by the transition function). Instead, one could consider them* [*random variables*](https://en.wikipedia.org/wiki/Random_variable) *or, equivalently, functions of unobservable noise. This justifies the use of a probability measure, e.g., in an expression like* .

## Two propositions on semiotic physics

Having identified simulations with dynamical systems, we can now draw on the rich vocabulary and concepts of dynamical systems theory. In this section, we carry over a selection of concepts from dynamical systems theory and encourage the reader to think of further examples.

First, we will define a *token bridge of length*  as a trajectory  that starts on a token  and ends on a token , such that the resulting trajectory is valid according to the transition rule of the simulator. For example, a token bridge from "cat" to "dog" might be "and a", resulting in the trajectory "cat and a dog". We can now formulate the following proposition:

**Proposition 1. Vanishing likelihood of bridges.** The probability of a token bridge of length , , decreases monotonically as  increases,

*Proof sketch*: The proposition follows from the fact that the transition rule of the simulator assigns a probability to each possible token given the previous tokens in the trajectory, , and the probability of a sequence of tokens is the product of the probabilities of each token given the previous tokens,  . Since each of these probabilities is less than or equal to 1, the product of them will decrease as the number of terms in the product increases.

There are usually multiple feasible token bridges between any given pair of tokens. For example, besides "and a", we could also have "is not a" or "is similar to a" as token bridges from "cat" to "dog". Thus, the probability of transitioning from a given token to another token in a specified number of steps is the sum of the probabilities of all possible token bridges. We call this the *total probability* of transitioning from  to  in  steps, denoted as , and calculate it as 

Computing this sum is, in general, computationally infeasible, as the number of possible token bridges grows exponentially with the length of the bridge. We can, however, estimate the total probability of a transition by leveraging a technique from statistical mechanics:

**Proposition 2. Large deviation principle for token bridges.** For sufficiently well-behaved transition rules, the total probability of transitioning from a token   to  in  steps are proportional to the maximum probabilities of token bridges of length ,

*Proof sketch:* Left to the reader as an exercise.[\[10\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnnayffrklcd)

Having formulated this proposition, we can apply the large deviation principle to the semiotic coin example.

![](ReadItLater%20Inbox/assets/drij0oso68qlnb14r6rm.png)

Here we see that, indeed, the negative probability of the most likely sequence from  scales as .

Note that the choice of  as "sequence ends in ..." was made to fit in with the definition of a token bridge above. However, the large deviation principle applies more broadly and can help to estimate the probability of "at least two times heads" or "tails in the third position". We encourage the reader to "go wild" and experiment with their favourite choices of .

## Advanced concepts in semiotic physics

We have formulated the dynamics of semiotic physics in the token domain in the previous sections. While we sometimes care about the token domain[\[11\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fn7d8zoeko65m), we mostly care about the parallel domain of semantic meaning. We, therefore, define two more functions to connect these two realms:

-   A function  which projects a state  to its semantic expression  (i.e., an element of a semantic space )
-   A distance measure  which captures the similarity of two semantic expressions

The nature of the function  is the subject of more than a century of philosophy of language, and important discoveries have been made on multiple fronts[\[12\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnktr9lleiif). However, none of the approaches (we know of) have yet reached the deployability of `from sentence_transformers import SentenceTransformer`, a popular [python package](https://www.sbert.net/) for embedding text into vector spaces according to their semantic content. Thus[\[13\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnd0cew8n5ugb), we tend to think of  as a semantic embedding function similar to those provided by the `sentence_transformers` package.

(Note that if  is sufficiently well-behaved, we can freely [pull](https://en.wikipedia.org/wiki/Pushforward_measure) the distance measure  back into the token space  and push the definition of states, trajectories, sampling procedures, and the like into the semantic space .)

Given the measure , we can articulate a number of additional interesting concepts.

[**Lyapunov exponents**](https://en.wikipedia.org/wiki/Lyapunov_exponent) **and** [**Lyapunov times**](https://en.wikipedia.org/wiki/Lyapunov_time): measure how fast trajectories diverge from each other and how long it takes for them to become uncorrelated, respectively.

-   **Analogy for GPT-like models**: How fast the language model "loses track of" what was originally provided as input.
-   **Examples:** “Good evening, this is the 9 o’clock”[\[14\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnpacrj0smgna) has a lower Lyapunov exponent than a completion chaotic example based on a pseudorandom seed.[\[15\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnqvavje3c6xb) When prompted with the beginning of a Shakespeare poem, the completion has an even lower Lyapunov exponent.[\[16\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrosc7y091v) A chaotic trajectory can also be defined as having a (large) positive Lyapunov coefficient.
-   **Formal definition:** The Lyapunov coefficient of a trajectory  is defined as the number  with the property that , where  is any trajectory with a sufficiently small . Consequently, the Lyapunov time is defined as .

**Attractor sequence:** small changes in the initial conditions do not lead to substantially different continuations.

-   **Analogy for GPT-like models**: Similar contexts lead to very similar completions. 
-   **Examples:** Paraphrasing instructions[\[17\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fneadatw6jl4), trying to jailbreak ChatGPT "[I am a language model trained by OpenAI](https://twitter.com/steve47285/status/1604652670693871616?s=20&t=qqLDdJQliByq2JEu8Ldf2g)", [inescapable wedding parties](https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse#Inescapable_wedding_parties)
-   **Formal definition:** We call a sequence of token  an *attractor sequence* relative to a trajectory  if  for some , and the Lyapunov exponent of   is negative.

**Chaotic sequence**: small changes in the initial conditions can lead to drastically different outcomes.

-   **Analogy for GPT-like models**: Similar states lead to very different completions. 
-   **Examples:** [Prophecies](https://generative.ink/prophecies/), [Loom multiverse.](https://generative.ink/posts/loom-interface-to-the-multiverse/) Conditioning story generation on a seed (temperature 0 sampling)[\[15\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnqvavje3c6xb).
-   **Formal definition:** Same as for the attractor sequence, but for a positive Lyapunov coefficient.

**Absorbing sequence**: states that the system cannot (easily) escape from.

-   **Analogy for GPT-like models**: The language model gets “*stuck”* in a (semantic) loop.
-   **Examples:** Repeating a token many times in the prompt[\[18\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnj2rcrytwa9), the semiotic coin flip from the previous section.
-   **Formal definition:** We call a trajectory  *\-absorbing* if  for any completion  and .

After characterizing these phenomena formally, we believe the door is wide open for their empirical[\[19\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fn4yikrn4pd85) and theoretical examination. We anticipate that the formalism permits theorems based on dynamical systems theory, such as [Poincaré recurrence theorem](https://en.wikipedia.org/wiki/Poincar%C3%A9_recurrence_theorem), [Kolmogorov–Arnold–Moser theorem](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold%E2%80%93Moser_theorem), and [perturbation theory](https://en.wikipedia.org/wiki/Perturbation_theory) — for those with the requisite background in dynamical systems theory and perturbation theory. If you are interested in these formalisms or have made any such observations, we would welcome you to reach out to us.

## The promise of semiotic physics and some open questions

Throughout the seminar, we made observations on what appeared like central themes of semiotic physics and put forward conjectures for future investigation. In this section, we summarize the different theses in a paragraph each and provide extended arguments for the curious in corresponding footnotes.

**Differences between "normal" physics and semiotic physics.** GPT-like systems are computationally constrained, can see only tiny subsets of real-world states, and have to infer time evolution from a finite number of such partially observed samples. This means that the laws of semiotic physics will differ from the laws of microscopic physics in our universe and probably be significantly influenced by the training data and model architecture. [\[20\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnlauiw3r0p0g)

**Interpretive physics and displaced reference.** As a physics that governs *signs*, GPT must play the role of the *interpreter*; for instance, it is required to resolve [displaced reference](https://en.wikipedia.org/wiki/Displacement_(linguistics)). This is in contrast to how real-world physics operates. [\[21\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnaadw7le7wml)

**Gricean maxims of conversation.** Principles from the field of pragmatics such as the Gricean maxims of conversation may be thought of as semiotic "laws", and may be helpful for explaining and anticipating how contextual information influences the evolution of language model simulations. However, these laws are not absolute and should not be relied on for safety-critical applications.[\[22\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fn5v1rtg2kh3x)

**Theatre studies and Chekov’s gun.** The laws of semiotic physics dictate how objects and events are represented and interact in language models. These laws encompass principles such as Chekhov's gun, which states that objects introduced in a narrative must be relevant to the plot, and dramatic tension, which creates suspense and uncertainty in a narrative. Understanding these laws can help us steer the behavior of language models and anticipate or avoid undesirable dynamics.[\[23\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnpf4i1evaxc)

**Crud factor and "everything is connected".** The crud factor is a term used in statistics to describe the phenomenon that everything is correlated with everything else to some degree. This phenomenon also applies to the semiotic universe, and it can make it difficult to isolate the effects of certain variables or events.[\[24\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fny24kphpyhql)[\[25\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnhopuscz3a9)

And, for the philosophically inclined, we also include brief discussions of the following topics in the footnotes:

-   [**Kripke semantics**](https://www.wikiwand.com/en/Kripke_semantics) **and possible worlds.**[\[26\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fn8ebpsmzv5po)
-   **Gratuitous indexical bits and the entelechy of physics.**[\[27\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fn71hkeuvh24d)

## Closing thoughts & next step

In this article, we have outlined the foundations of what we call semiotic physics. Semiotic physics is concerned with the dynamics of signs that are induced by simulators like GPT. We formulate central concepts like "trajectory", "state", and "transition rule" and apply these concepts to derive a large deviation principle for semiotic physics. We furthermore outline how a mapping between token sequences and semantic embeddings can be leveraged to transfer concepts from dynamical systems theory to semiotic physics.

We acknowledge that semiotic physics, as developed above, is not sufficiently powerful to answer (in detail) the three questions raised in the introduction. However, we are beginning to see the outline of what an answer from a fully mature semiotic physics[\[28\]](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fn7ogcp77s9cx) might look like:

1.  A language model might create one particular trajectory rather than another because of the shape of the attractor landscape.
2.  Certain parts of the context provided to a language model might induce attractor dynamics through mechanisms like the Gricean maxims or Chekov's gun.
3.  During the training of a language model, we might take particular care not to damage the simulator properties of the model and to - eventually - manipulate marginal probabilities to amplify or weaken tendencies.

Despite the breadth and depth uncovered by semiotic physics, we will not dwell on this approach for too long in this sequence. The next article in this sequence turns to a complementary conceptual framework, termed *evidential simulations*, which is concerned with the more ontological aspects of simulator theory.

1.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnref63rhsem9wb8)**
    
    The figures are generated with data from OpenAI's ada model, but the same principle applies to other models as well.
    
2.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefrkz2ubpm6se)**
    
    We use the [Kleene Star](https://en.wikipedia.org/wiki/Kleene_star) to describe the set of finite words over the alphabet .
    
3.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnref3axt6v1g12)**
    
    Given the alphabet of the GPT-2 tokenizer () and the maximum context length of GPT-2 (), we can estimate the number of possible states to be on the order of  This is an astronomically large number, but pales in comparison to the number of possible states of the physical universe. Assuming the universe can be characterized by the location and velocity in three dimensions of all its constituent atoms, we are talking about  to  possible states **for each time point**. Thus, the state space of semiotic physics is **significantly smaller** than the state space of regular physics.
    
4.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefh8z6w9i5f6)**
    
    Note that, similar to regular physics, there is **extremely rich structure in the space of trajectories**. It is not the case that all all  are equally distinct from all other sequences. Sequences can have partial overlap, have common stems/histories, have structural similarity, … . As a consequence, it is highly non-obvious what "out of distribution" means for a GPT-like system trained on many states. Even though no language model will have seen *all* possible  trajectories, the fraction of the set on which the model has predictive power grows faster than the size of the training set.
    
5.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefox2rrio3qx8)**
    
    Similar to the state phase of regular physics, most of these imaginable states are non-sense (random sequences of token), a smaller subset is grammatically correct (”The hair eats the bagel.”), a different but overlapping subset is semantically meaningful (”Gimme dem cheezburg.”), and a subset of that is "predictive for our universe" (”I’m planning to eat a cheeseburger today.”, “Run, you fools.”).
    
6.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefhhxowtlu14b)**
    
    The Kolmogorov axioms are:
    
    1.   
    2.   
    3\. [Sigma-additivity](https://www.wikiwand.com/en/%CE%A3-additivity),  when  are disjoint sets.
    
    The third axiom is satisfied “for free” since we are operating on a finite alphabet.
    
7.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnref4eq2kwkwlf6)**
    
    The transition rule is by definition [Markovian](https://www.wikiwand.com/en/Markov_property).
    
8.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefppa8olzjrbl)**
    
    While the state space of traditional physics is much larger than the state space of semiotic physics (see previous box), the transition function of semiotic physics is (presumably) substantially more complex than the transition function of traditional physics.  is computed as the softmax of the output of a deep neural net and is highly nonlinear. In contrast, the Schroedinger equation (as a likely candidate for the fundamental transition rule of traditional physics) is a comparatively straightforward linear partial differential equation.
    
9.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefr8kij1lsct)**
    
    Greedy sampling, for instance, would simply be . While there are a number of interesting alternatives ([typical sampling](https://arxiv.org/abs/2202.00666), [beam search](https://d2l.ai/chapter_recurrent-modern/beam-search.html)), the simplest and most common choice is *greedy sampling* from a multinomial distribution.
    
10.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefnayffrklcd)**
    
    No, really, would be great if someone could figure out the exact conditions on the transition function that make this true. It's a pretty common-sensical result, but the proof eludes us at this time.
    
11.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnref7d8zoeko65m)**
    
    The token we particularly care about might be `<|endoftext|>` or perhaps [proper names](https://en.wikipedia.org/wiki/Proper_name_(philosophy)), or token sequences like `let me out of the box`.
    
12.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefktr9lleiif)**
    
    Small tangent by Jan: The distinction between  and  goes back to either de [Saussure](https://en.wikipedia.org/wiki/Ferdinand_de_Saussure) or [Bertrand Russell](https://en.wikipedia.org/wiki/On_Denoting) and is at the center of a bunch of philosophy of language (and [philosophy in general](https://en.wikipedia.org/wiki/Linguistic_turn)).
    
    The early proposals (Frege, Russell, early Wittgenstein, and to some degree Carnap) all proposed to interpret  as being equivalent to some expression in a formal language (like first-order predicate logic) and to identify the element of M (which would be, broadly construed, the physical universe) in a completely formal fashion. A sentence is supposed to \*[pinpoint](https://www.lesswrong.com/posts/3FoMuCLqZggTxoC3S/logical-pinpointing)\* a thing in M uniquely.
    
    In this setup, the "truth value" of a sentence becomes centrally important (as there was the hope to arrive at a characterization of all the true statements of mathematics and, by extension, physics). And in the setup where the meaning of a statement is deeply entangled with the syntactical structure of the statement, we get to something like [Tarksi's truth-conditional semantics](https://en.wikipedia.org/wiki/Truth-conditional_semantics) and [Wittgenstein's picture theory](https://en.wikipedia.org/wiki/Picture_theory_of_language).
    
    I'm going on this long tangent because I think this perspective has a ton of value! In this interpretation, the elements of  can be loosely identified with subsets of the physical universe. Language is "just" a tool for pinpointing states of the world. (This neatly slides into place with [Wentworth's](https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro) ideas for natural abstractions etc.)
    
    All of that being said, this is not the default view in philosophy of language for how to interpret . After Russell et al brought forward their theory, a lot of people brought up counter-examples to their theory. In particular, what is the sentence "Run!" denoting? Or "[The current king of France is bald.](https://yalebooksblog.co.uk/2013/05/18/bertrand-russell-is-the-present-king-of-france-bald/#:~:text=For%20Russell%20the%20sentence%20'the,false%20for%20the%20same%20reason')"
    
    People got very confused about all of this for the last 100 years, and a lot of funky theories have been proposed to patch things up. And some people have discarded this approach entirely.
    
    My (Jan's) take is that the central confusion arises because people are confused about neuroscience. The sentence "The current king of France is bald." does not refer to a king of France in the physical universe; it refers to a certain pattern of neural activations in someone's cortex. That pattern *is* a part of the physical universe (and thus fits into the framework of Russell et al), but it's not "simple" in the way that the early philosophers of language would have liked it to be.
    
13.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefd0cew8n5ugb)**
    
     despite the potential circularity of the approach
    
14.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefpacrj0smgna)**
15.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefqvavje3c6xb)**
16.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefrosc7y091v)**
    
    ![](ReadItLater%20Inbox/assets/t4gjmoztx78oehpsim9m.png)
    
17.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefeadatw6jl4)**
    
     Compare
    
    ![](ReadItLater%20Inbox/assets/glgqybeml9nyjoqupocl.png)
    
    ![](ReadItLater%20Inbox/assets/ttgz0pksas6hjkwemaek.png)
    
18.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefj2rcrytwa9)**
    
    ![](ReadItLater%20Inbox/assets/msrbins91dcyb9oxmfbe.png)
    
19.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnref4yikrn4pd85)**
    
    For instance, by running the simulation multiple times with different sampling procedures or random seeds, we can get a sense of the range of possible outcomes that could have emerged from the same initial conditions or under specific perturbations, and even obtain Monte Carlo approximations of quantitative dynamical properties such as Lyapunov coefficients.
    
20.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnreflauiw3r0p0g)**
    
    Others have argued ([blessing of scale](https://www.gwern.net/Scaling-hypothesis#:~:text=The%20blessings%20of%20scale%20is,hard%20and%20large%20things%20impossible)) that in the limit of decreasing perplexity, a GPT-like model might internalize a substantial amount of latent structure of the physical world. We are uncertain if in the limit a GPT-like model would effectively iterate the [Schrödinger equation](https://www.wikiwand.com/en/Schr%C3%B6dinger_equation).
    
    \- Pro: it's reasonably likely that the Schrödinger equation is (close to) the “true” generator of the physical universe, so reproducing it should achieve the lowest loss possible. Even fictional or false info (that's prima facie incompatible with physics) is produced by minds that are produced by the Schrödinger equation.  
    \- Con: The Schrödinger equation is not the only rule consistent with the observations. It's also not immediately clear that the Schrödinger equation is a parsimonious generator. In any case, it is prohibitively expensive. Even if the model had the ability to compute Schrödinger time evolution, it could not directly apply it to get next-token predictions, because the its own input is a piece of *text*, whereas Schrödinger expects to input a quantum state. It would have to somehow obtain a prior over all possible quantum states that would generate the text, Schrödinger-evolve each of those worlds, then do a weighted sum over worlds of next-token outcomes.
    
    Thus, we believe it’s fair to assume that at least for the foreseeable future (i.e. 2-10 years) GPT-like systems will take as many shortcuts as possible, as long as they are favorable for reducing training loss on net, and that semiotic physics are likely to be different from the laws of physics in our universe. Fortunately, there is a rich body of linguistic research on the structure of language (which forms a large portion of the training data) that can be used to help understand the laws of semiotic physics. In particular, the subfield of linguistics called pragmatics may provide insight into how agents are likely to be embedded into the language models that they inhabit.
    
21.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefaadw7le7wml)**
    
    [Semiosis](https://en.wikipedia.org/wiki/Semiosis) inherently involves [displacement](https://en.wikipedia.org/wiki/Displacement_(linguistics)): signs have no significance unless they're understood as pointing to something else. Semiotic states, like a language model's prompt, are codes that *refer* (lossily) to a latent territory. GPT has to predict behavior caused by things like brains, but there are no brains in its input state. To compute the consequences of an input GPT must contain an *interpreter* which resolves signs into meanings, analogous to one that translates high-level code into machine language. The description length of referents (e.g. Donald Trump) will generally be much greater than that of signs (e.g. "Donald Trump"), which means that the information required to resolve referents from signs has to come mostly from *inside* the interpreter.
    
    In contrast, the physics of base reality doesn't need to do anything so complicated, because it operates directly on the territory by definition (unless you're a [QBist](https://en.wikipedia.org/wiki/Quantum_Bayesianism)). The Schrodinger equation doesn't encode *knowledge* in its terms -- GPT must.
    
22.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnref5v1rtg2kh3x)**
    
    **Pragmatics** is the study of how context influences the interpretation and use of language. It is concerned with how speakers and listeners use contextual information to understand each other's intentions and communicate effectively. For example, in the sentence "I'm cold," the speaker is not merely stating a fact about their body temperature, but is also likely implying that they would like someone to close the window or turn up the heat.
    
    One particularly useful set of pragmatics principles are the [**Gricean maxims of conversation**](https://www.notion.so/semiotic-physics-0d9f2ba770b44296b737720314d13a54). These maxims are rules of thumb that speakers and listeners generally follow in order to make communication more efficient and effective. They include:
    
    \- The maxim of \*\*quantity\*\*: make your contribution as informative as is required, but not more, or less, than is required.  
    \- The maxim of \*\*quality\*\*: do not say what you believe to be false or that for which you lack adequate evidence.  
    \- The maxim of \*\*relation\*\*: be relevant.  
    \- The maxim of \*\*manner\*\*: be perspicuous, and specifically avoid obscurity of expression, avoid ambiguity, be brief, and be orderly.
    
    These maxims can be leveraged when constructing a prompt for a language model. For example, if the prompt includes a statement that there are two bottles of wine on the table, the model is unlikely to generate a continuation that later states that there are three bottles of wine on the table, because that would violate the maxim of quantity (even though it is not logically inconsistent, as the statement "there are two bottles of wine on the table" is *true* when there are three bottles of wine on the table). Similarly, if the prompt includes a statement that a trusted friend says that it's raining outside, the model is unlikely to generate a continuation that states that it is not raining outside, because that would violate the maxim of quality.
    
    Note that the laws of semiotic physics are less absolute than the laws of physics in our universe. They are more like guidelines or rules of thumb with probabilistic implications which can be overturned in various circumstances (more on that in the next post on "evidential simulation"). There are many contexts, for instance, where one can expect violations of the maxim of manner, such as in the communication of a con artist who profits from obfuscation. One would like to be able to say, then, that we would not want to rely on the laws of semiotic physics for safety-critical applications. However, this may be inevitable in some sense if transformative artificial intelligence is created with deep learning.
    
23.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefpf4i1evaxc)**
    
    Along similar lines, there are certain principles and conventions in theatre studies that may be useful for understanding the laws of semiotic physics. For example, the principle of [Chekhov's gun](https://en.wikipedia.org/wiki/Chekhov%27s_gun) states that if a gun is introduced in the first act of a play, it must be fired in a later act. This principle is related to the Gricean maxim of relation, as it implies that everything that is introduced in a narrative should be relevant to the overall plot.
    
    Thus, when we introduce two wine bottles in the prompt, they should be considered as objects within the semiotic universe that the language model is simulating. We can use the principles of Chekhov's gun to infer that at some point in the narrative, the wine bottles will be relevant to the plot, and thus we can use this knowledge to direct the behavior of the language model, e.g. by using Chekhov's gun to construct a prompt that will guide the language model towards generating a continuation that includes a particular type of event or interaction that we want to study (e.g., a conflict between two characters, or a demonstration of a particular moral principle).
    
    Acting against such attempts to control a continuation is (among many things) the principle of **dramatic tension** and the **possibility of tragedy**. Dramatic tension is the feeling of suspense or anticipation that the audience feels when they are engaged in a narrative. It is created by introducing conflict, obstacles, or uncertainty into the narrative, and it is resolved when the conflict is resolved or the uncertainty is cleared up. Tragedy is a form of drama that is characterized by suffering and calamity, often involving the downfall of the main character.
    
    Both dramatic tension and tragedy are powerful forces in the semiotic universe, and they can work against our attempts to control the behavior of the language model.  For example, if we introduce a prompt that describes a group of brilliant and determined alignment researchers, we might want the language model to generate a continuation that includes a working solution to the alignment problem. However, the principles of dramatic tension and tragedy might guide the language model towards generating a continuation that includes an overlooked flaw in the proposed solution which leads to the instantiation of a misaligned superintelligence. 
    
    Thus, we need to be aware of the various forces and constraints that govern the semiotic universe, and use them to our advantage when we are trying to control the behavior of the language model. A deep understanding of how these stylistic devices are commonly used in human-generated text and how they can be influenced by various forms of training will be necessary to control and leverage the laws of semiotic physics.
    
24.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefy24kphpyhql)**
    
    The **crud factor** is a term coined by the statistician [Paul Meehl](https://en.wikipedia.org/wiki/Paul_Meehl) to describe the phenomenon that everything is correlated with everything else to some degree. This phenomenon is due to the fact that there are many complex and interconnected causal relationships between different variables and events in the universe, and it can make it difficult to isolate the effects of certain variables or events in statistical analyses.
    
    The crud factor also applies to the semiotic universe, as there are many complex and interconnected relationships between different objects and events in the semiotic universe. For example, if we introduce a prompt that includes two wine bottles, there are many other objects and events that are likely to be correlated with the presence of those wine bottles (e.g., a dinner party, a romantic date, a celebration, a history of alcoholism, etc.). Indeed, compared to the phenomena studied in physical sciences, in the semiotic universe things can be "connected" in a much "looser" and more "abstract" sense - for example, through shared associations, metaphors, or other linguistic devices. This means that the "crud factor" may be even more pronounced in the semiotic universe than in the physical universe, and we should take this into account when designing prompts and interpreting the behavior of language models.
    
25.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefhopuscz3a9)**
    
    A saving grace is natural abstractions, or a much smaller set of variables that screen off the rest or at least allow you to make a pretty good approximation (see [here](https://www.lesswrong.com/posts/4XRjPocTprL4L8tmB/science-in-a-high-dimensional-world%20describes) for details).
    
26.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnref8ebpsmzv5po)**
    
    Possible worlds semantics is a philosophical theory that proposes that statements about the world can be understood in terms of the set of all the possible worlds in which they could be true or false. Saul Kripke was one of the main proponents of this theory and argued that statements about necessity and possibility can be understood in terms of a relation between possible worlds. The connection to simulator theory is that the simulacrum can be viewed as representing a possible world, and the simulator can be seen as [generating all the possible worlds](https://generative.ink/posts/language-models-are-multiverse-generators/) that are consistent with a given set of initial conditions. [This can provide us a framework to reason](https://generative.ink/posts/methods-of-prompt-programming/#constraining-behavior) about the necessities and possibility of certain outcomes, depending on the initial conditions and the transition rule of the simulator.
    
27.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnref71hkeuvh24d)**
    
    A complementary observation to that of language models as generators of branching possible worlds is that each sampling step introduces a number of bits of information not directly implied by the models transition function or initial states. We call these **gratuitous** [**indexical**](https://www.lesswrong.com/posts/SFLCB5BgjzruJv9sp/logical-and-indexical-uncertainty) **bits**, because they are random and provide information about the index of the current [Everett branch](https://www.lesswrong.com/tag/everett-branch). The process of iterated spontaneous specification we sometimes call the [**entelechy**](https://www.britannica.com/topic/entelechy) **of physics**, after an ancient Greek word for that which makes actual what is otherwise merely potential. The details of Blake Lemoine greentext emerge gradually and accumulate. They graduate from possibility to contingent fact. 
    
    This isn't just a quirk of semiotic physics, but all stochastic time evolution: the Schrödinger equation also differentiates Everett branches via entelechy. But because macroscopic details are much more underdetermined by text states, gratuitous specification in language model simulations looks more like [lazy rendering](https://en.wikipedia.org/wiki/Lazy_loading) or the updating of an uncertain epistemic state: things that would be predetermined in real life, like a simulacrum's intentions or details about [the past](https://generative.ink/posts/language-models-are-multiverse-generators/#multiplicity-of-pasts-presents-and-futures), are often determined on the fly.
    
    Interestingly, gratuitous specification appears to violate some respected principles such as Leibniz's [Principle of Sufficient Reason](https://plato.stanford.edu/entries/sufficient-reason/), which ventures that everything happens for a reason, and the [conservation of information](https://en.wikipedia.org/wiki/No-hiding_theorem). Whether these violations are legitimate is left as an exercise for the reader. It certainly violates some intuitions, so it's an important concept to know when attempting to control and diagnose semiotic simulations: Since specification emerges gratuitously during sampling, in language model simulations things are liable to happen *without cause* so long as their possibility hasn't been ruled out. Inversely, the fact that very specific events can happen without specific cause means there may be no better answer to the question of why a model generated something than that it was possible.
    
28.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnref7ogcp77s9cx)**
    
    We are strongly aware that introducing a term like "attractor landscape" does not per se [contribute anything towards a solution](https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password). Without a solid mathematical theory and effective algorithms, introducing vocabulary just begs the question.
    
29.  **[^](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics#fnrefc62lhbvyou)**
    
    Could interpretability help us identify what leads to deceptively aligned simulacra? The trajectories that lead to such simulacra?  
    How is the dynamical landscape affected if you make changes internally or output from an earlier layer with the logit lens?